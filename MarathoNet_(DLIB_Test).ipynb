{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MarathoNet\n",
    "\n",
    "_MarathoNet_ es un sistema inteligente capaz de detectar el número de dorsal de corredores de maratón a partir del video extraído de una cámara que graba la carrera en un punto de control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema\n",
    "\n",
    "Para poder identificar el dorsal de un corredor necesitamos detectar en tiempo real dónde se encuentra el cuerpo de éste en cada frame que extraemos de un vídeo que graba la carrera. \n",
    "\n",
    "En este notebook se plantea utilizar la librería _DLIB_ preentrenado con un modelo para la identificación de puntos clave de la cara. Los cuales podrán ser de utilidad para diseñar un subsistema de identificación y tracking de corredores de maratón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas iniciales con _DLIB_\n",
    "\n",
    "Con el fin de familiarizarnos con la librería [_DLIB_](http://dlib.net/), disponemos de una serie de vídeos de personas moviéndose en una escena que utilizaremos como toma de contacto inicial con la librería y así observar todas la potencia y funcionalidades que esta ofrece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de vídeos\n",
    "\n",
    "Se procede con la carga de los diferentes archivos de vídeo que disponemos para realizar las pruebas con [_DLIB_](http://dlib.net/). Para ello, importaremos el módulo [_os_](https://docs.python.org/3/library/os.html) que permite el manejo de diferentes funcionalidades de sistema operativo, entre ellas, el manejo de ficheros y directorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'videos'\n",
    "video_files = list()\n",
    "for root, dirs, files in os.walk(os.path.relpath(PATH)):\n",
    "    for file in files:\n",
    "        video_files.append(os.path.join(root, file))\n",
    "\n",
    "no_videos = len(video_files)\n",
    "print(video_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El manejo y procesado de videos será gestionado por la librería de visión por computador, [_OpenCV_](https://opencv.org/), que depende a su vez de la archiconocida librería de computación científica, [_Numpy_](https://www.numpy.org/). Mostraremos las imágenes que procesamos [_OpenCV_](https://opencv.org/) en el propio _Jupyter Notebook_, es por este motivo por el cuál también tendremos que importar la librería [Matplotlib](https://matplotlib.org/). A continuación, procederemos con la carga de sendos paquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto _VideoCapture_ es el que recibe la entrada de vídeo. Este dispone de una serie de métodos que iremos utilizando a lo largo de las pruebas con [_MTCNN_](http://dlib.net/). Se crea un diccionario que contenga un objeto _VideoCapture_ para cada imagen que encontremos en el directorio donde almacenamos los archivos de vídeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [ 'video_' + str(i+1) for i in range(len(video_files)) ]\n",
    "\n",
    "videos = { k: cv.VideoCapture(cv.samples.findFileOrKeep(v)) for (k, v) in zip(keys, video_files) }\n",
    "\n",
    "for k,v in videos.items():\n",
    "    if not v.isOpened():\n",
    "        print('Unable to open: \\'' + k + '\\'')\n",
    "        exit(0)\n",
    "\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionaremos un frame arbitrario con el método _set()_ de cada objeto de la clase _VideoCapture_ generado en el paso anterior. El número máximo de frames de un video lo extraemos con el método _get()_ y el selector _cv.CAP_PROP_FRAME_COUNT_ de la clase _VideoCapture_. \n",
    "\n",
    "Tomamos un número entero aleatorio dentro del rango de frames que dispone cada vídeo para poder mostrar un frame aleatorio de la misma. Por este motivo tendremos que utilizar el método _randint()_ e importar la librería [random](https://docs.python.org/2/library/random.html).\n",
    "\n",
    "Posteriormente, recuperamos el frame con el método _read()_ de la misma clase y lo mostraremos por pantalla utilizando el método _plt.imshow()_ de la librería [Matplotlib](https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "for i in range(no_videos):\n",
    "    total_frames = videos['video_'+str(i+1)].get(cv.CAP_PROP_FRAME_COUNT)\n",
    "    actual_frame = randint(0, total_frames)\n",
    "    \n",
    "    videos['video_'+str(i+1)].set(cv.CAP_PROP_POS_FRAMES, actual_frame)\n",
    "    \n",
    "    ret, frame = videos['video_'+str(i+1)].read()\n",
    "    img = cv.cvtColor(frame, cv.COLOR_BGR2RGBA)\n",
    "    h,w = img.shape[:2]\n",
    "    \n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia de puntos clave de la cara con _DLIB_\n",
    "\n",
    "La librería [_DLIB_](http://dlib.net/) utiliza una red neuronal convolutiva que utiliza un método denominado [_Maximum-Margin Object Detection_](https://arxiv.org/pdf/1502.00046.pdf). Este modelo ha sido preentrenado con datasets como _ImageNet_, _PASCAL VOC_, _VGG_, _WIDER_, _Face Scrub_. El modelo preentrenado puede ser descargado del [repositorio oficial](https://github.com/davisking/dlib-models) de [_DLIB_](http://dlib.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a realizar la carga del modelo preentreando para la detección de regiones faciales y para la predicción de puntos clave de la cara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_DETECTOR_PATH = \"models/mmod_human_face_detector/mmod_human_face_detector.dat\"\n",
    "FACE_PREDICTOR_PATH = \"models/shape_predictor_68_face_landmarks/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "detector = dlib.cnn_face_detection_model_v1(FACE_DETECTOR_PATH)\n",
    "predictor = dlib.shape_predictor(FACE_PREDICTOR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redimensión de frames de vídeo\n",
    "\n",
    "Se procederá a implementar una pequeña función que que toma como entrada un frame de vídeo y lo redimensionará, conservando la proporción del mismo, a un tamaño máximo de 800x600 píxeles, siempre y cuando el frame de entrada supere dichas dimensiones. La redimensión de la imagen conlleva una interpolación de área de la imagen resultante (_cv.INTER_\\__AREA_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_size(inSize, outSize=(800,600)):\n",
    "    w, h = inSize\n",
    "    \n",
    "    if w > outSize[0]:\n",
    "        scaling = outSize[0] / w\n",
    "        w = int(w * scaling)\n",
    "        h = int(h * scaling)\n",
    "        \n",
    "    if h > outSize[1]:\n",
    "        scaling = outSize[1] / h\n",
    "        w = int(w * scaling)\n",
    "        h = int(h * scaling)\n",
    "        \n",
    "    return (w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frame(frame, inSize, outSize=(800,600)):\n",
    "    dim = define_size(inSize, outSize)\n",
    "    \n",
    "    return cv.resize(frame, dim, interpolation=cv.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector de cara con _DLIB_\n",
    "\n",
    "Se procede a implementar una pequeña función responsable de detectar regiones donde encontrar una cara de un sujeto dentro de cada fotograma de vídeo. Cada frame será reescalado antes de ser identificada la región en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infere_region(frame, detector, upsampling, inSize, outSize=(800,600)):\n",
    "    return detector(resize_frame(frame, inSize, outSize), upsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infiriendo puntos clave de la cara con _DLIB_\n",
    "\n",
    "A continuación se procede con la inferencia de puntos clave de la cara. Se devuelve un diccionario por fotograma procesado. Este diccionario tiene la estructura de salida del fichero _JSON_ sobre el cual se guardan los resultados de los puntos inferidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infere_keypoints(frame, faces, predictor):\n",
    "    data = { 'people': list() }\n",
    "    \n",
    "    for i, face in enumerate(faces):\n",
    "        kp = predictor(frame, face.rect)\n",
    "        data['people'].append( {'face_keypoints_2d': list()} )\n",
    "        \n",
    "        for j in range(kp.num_parts):\n",
    "            data['people'][i]['face_keypoints_2d'].extend( [kp.part(j).x, kp.part(j).y] )\n",
    "        \n",
    "    return data        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generando resultados\n",
    "\n",
    "El objetivo de este apartado consiste en generar un un fichero _JSON_ por fotograma de vídeo con toda la información relativa a los puntos de interés de la cara detectada por [_DLIB_](http://dlib.net/). Deberemos importar además él módulo [_json_](https://docs.python.org/2/library/json.html) de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "d1 = today.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "OUT_PATH = \"out/dlib_\" + d1 + \"/\"\n",
    "\n",
    "if not os.path.exists(OUT_PATH):\n",
    "    os.mkdir(OUT_PATH)\n",
    "\n",
    "for i, file_i in enumerate(video_files):\n",
    "    file_i = os.path.splitext(os.path.split(file_i)[1])[0]\n",
    "    \n",
    "    if not os.path.exists(OUT_PATH + \"video/\"):\n",
    "        os.mkdir(OUT_PATH + \"video/\")\n",
    "        \n",
    "    if not os.path.exists(OUT_PATH + \"json/\"):\n",
    "        os.mkdir(OUT_PATH + \"json/\")\n",
    "    \n",
    "    if not os.path.exists(OUT_PATH + \"video/\" + file_i):\n",
    "        os.mkdir(OUT_PATH + \"video/\" + file_i)\n",
    "    \n",
    "    if not os.path.exists(OUT_PATH +\"json/\" + file_i):\n",
    "        os.mkdir(OUT_PATH + \"json/\" + file_i)\n",
    "    \n",
    "    total_frames = int(videos['video_'+str(i+1)].get(cv.CAP_PROP_FRAME_COUNT))\n",
    "    w = int(videos['video_'+str(i+1)].get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(videos['video_'+str(i+1)].get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    videos['video_'+str(i+1)].set(cv.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    for j in range(total_frames):\n",
    "        ret, frame = videos['video_'+str(i+1)].read()\n",
    "        faces = infere_region(frame, detector, 1, (w,h))\n",
    "        data = infere_keypoints(frame, faces, predictor)\n",
    "        \n",
    "        json_name = f'dlib_{file_i}_{j:09}.json'\n",
    "\n",
    "        with open(OUT_PATH + \"json/\" + file_i + \"/\" + json_name, \"w\") as write_file:\n",
    "            json.dump(data, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
